---
title: Resources
title-prefix: Besim Kabashi
author: Besim Kabashi
lang: en
bibliography: kabashi_bibliography.bib
csl: chicago-author-date.csl
---

- [Data Sets](#data-sets)
- [Corpora](#corpora)


## Data Sets ##

### EmpiriST Corpus 2.0 ###

The EmpiriST Corpus 2.0 is a manually annotated corpus consisting of
German web pages and German computer-mediated communication (CMC),
i.e. written discourse. Examples for CMC genres are monologic and
dialogic tweets, social and professional chats, threads from Wikipedia
talk pages, WhatsApp interactions and blog comments.

The dataset was originally created by [Beißwenger et
al. (2016)](https://www.aclweb.org/anthology/W16-2606/) for the
[EmpiriST 2015](https://sites.google.com/site/empirist2015/) shared
task and featured manual tokenization and part-of-speech
tagging. Subsequently, [Rehbein et al.
(2018)](https://www.oeaw.ac.at/fileadmin/subsites/academiaecorpora/PDF/konvens18_03.pdf)
incorporated the dataset into their [harmonised testsuite for POS
tagging of German social media
data](https://www.cl.uni-heidelberg.de/~rehbein/tweeDe.mhtml),
manually added sentence boundaries and automatically mapped the
part-of-speech tags to [UD pos
tags](https://universaldependencies.org/u/pos/all.html). In our own
annotation efforts (Proisl et al., in preparation), we manually
normalized and lemmatized the data and converted the corpus into a
“vertical” format suitable for importing into the Open Corpus
Workbench, CQPweb, SketchEngine, or similar corpus tools.

Normalization and lemmatization added in collaboration with [Thomas
Proisl](https://thomas-proisl.de), Natalie Dykes, [Philipp
Heinrich](https://philipp-heinrich.eu/), and [Stefan
Evert](http://stefan-evert.de/).

-> See the data set (and the description) on
[GitHub](https://github.com/fau-klue/empirist-corpus).

## Corpora ##


### GeRedE: A Corpus of German Reddit Exchanges ###

GeRedE is a 270 million token German CMC corpus containing
approximately 380,000 submissions and 6,800,000 comments posted on
Reddit between 2010 and 2018.

Created in collaboration with Andreas Blombach, Natalie Dykes,
[Philipp Heinrich](https://philipp-heinrich.eu/) and [Thomas
Proisl](https://thomas-proisl.de/).

-> See the data set (and the description) on
[GitHub](https://github.com/fau-klue/german-reddit-korpus).


### Albanian Corpus (AlCo) ###

The Albanian Corpus (AlCo) contains a hundred million word tokens
(text words), the first Albanian corpus of this size. The corpus
covers different domains of language and contains different text types
– it is a reference corpus. At this moment the work is still in
progress, some texts still need to be replaced or recategorized. The
corpus is annotated with a morpho-syntactic tagset of 77 tags, since
2015. We use CQPweb, a web-based corpus analysis system, to explore
the corpus data.


### AlCoPress (2017-2019) ###

The Albanian Corpus of Press Texts (AlCo) contains around 32 million
word tokens (text words). The corpus is annotated like AlCo. We use
CQPweb, a web-based corpus analysis system, to explore the corpus
data.


### Buzuku (1555) Corpus ###

The Buzuku Corpus contains the text of "Missale" (1555) from Gjon
Buzuku.  The corpus is not annotated.



<!-- ## News ## -->